{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for the working (biomedical) researcher\n",
    "## <center>ELIXIR-IIB Machine Learning for Biologists 2017</center>\n",
    "### Marco Chierici\n",
    "#### _Data scientist @ FBK/MPBA_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this handout we will go through basic concepts of machine learning using Scikit-learn and the SEQC neuroblastoma data set [Zhang et al, _Genome Biology_, 2015]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/zhang.png\" width=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we will focus on **a subset of 272 samples (136 training, 136 test)**, aiming at predicting an **extreme disease outcome** (favorable vs unfavorable samples: see main paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/zhang_tab2.png\" width=\"95%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training set data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tr = pd.read_csv(\"data/nb_train.txt.gz\", dtype=str, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the data. First, the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's inside?\n",
    "\n",
    "A peek at the first rows reveals that the first column (the dataframe **index**) contains the sample IDs, the second column is the class (or target), and the remaining columns are genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the sample IDs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining part of this hands-on, we need the data to be stored in a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This keeps the class column as well.\n",
    "\n",
    "But...\n",
    "\n",
    "We need to separate the class from the data, so let's recreate our array by dropping the class column from the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.values[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the dtype to float and save the array to x_tr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tr = data_tr.values[:, 1:].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When coding, it is a good practice to have a peek at the resulting variables, to be sure everything is OK: i.e., is that variable like it is supposed to be? Did I accidentally throw away a feature column?\n",
    "\n",
    "This can avoid lots of problems later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the class column in another Numpy integer array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = data_tr[\"class\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Quick recap\n",
    "\n",
    "- y_tr = 1 indicates **unfavorable** neuroblastoma samples (**bad** outcome)\n",
    "- y_tr = -1 indicates **favorable** neuroblastoma samples (**good** outcome)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature and sample names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_tr = data_tr.columns[1:].values.astype(str)\n",
    "samp_tr = data_tr.index.values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the test set data in a Pandas dataframe and create the Numpy arrays for data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load sol1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, let's perform an **unsupervised learning** task on the data set \"as is\" by decomposing it in its Principal Components.\n",
    "\n",
    "Instantiate a scikit-learn PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll _fit_ the PCA object on the training data, at the same time _transforming_ them in the Principal Component space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_tr = pca.fit_transform(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the _variance ratio_, i.e. the percentage of the variance explained by each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What can you understand from these variance percentages? Could this task be \"predictable\" by some sort of model?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it always convenient to visualize the first two principal components in a scatterplot, in order to get a first assessment of the goodness of the decomposition.\n",
    "\n",
    "We will color the points in the plot according to our sample labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(z_tr[y_tr == 1, 0], z_tr[y_tr == 1, 1], color=\"r\")\n",
    "plt.scatter(z_tr[y_tr == -1, 0], z_tr[y_tr == -1, 1], color=\"b\")\n",
    "plt.title(\"PCA of Train data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a PCA plot, it is usually more informative to print also the explained variances (e.g., in the axis labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = pca.explained_variance_ratio_\n",
    "plt.figure()\n",
    "plt.scatter(z_tr[y_tr == 1, 0], z_tr[y_tr == 1, 1], color=\"r\")\n",
    "plt.scatter(z_tr[y_tr == -1, 0], z_tr[y_tr == -1, 1], color=\"b\")\n",
    "plt.title(\"PCA of Train data\")\n",
    "plt.xlabel(\"PC1 (%.2f%%)\" % (100*vars[0]))\n",
    "plt.ylabel(\"PC2 (%.2f%%)\" % (100*vars[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Now apply the transformation to the test data and plot it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load sol2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the PCA we built on our data, we decide to try some supervised learning on them.\n",
    "\n",
    "Scikit-learn provides you access to several models via a very convenient _fit_ and _predict_ interface.\n",
    "\n",
    "For example, let's fit a Random Forest classifier on the whole training data and then use this model to predict the labels of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500) # instantiate a RandomForestClassifier with a custom parameter\n",
    "clf.fit(x_tr, y_tr) # fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_ts) # predict labels on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to try a Support Vector Machine instead?\n",
    "\n",
    "In scikit-learn it is as easy as choosing the appropriate class, i.e. **SVC** (Support Vector Classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit and predict by using the same code as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC() # instantiate a SVC object with default parameters\n",
    "clf.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_OK, I (over)simplified a bit here. In general, a classifier has hyper-**parameters** that need to be tuned. Default choices are not good in all situations._\n",
    "\n",
    "_For example, in SVC the two main parameters are the **regularization** parameter C and the **kernel** function (linear, gaussian, polynomial, etc.)_\n",
    "\n",
    "_Usually we search the parameter space (**Grid Search**) for the combination that yields the best Cross-Validation score_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our previously trained Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "clf.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **most important features**? i.e., those having a greater power in discriminating the two sample classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to rank features according to some classifier properties, such as Random Forest importances or Linear SVM weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the features according to the Random Forest importances (Gini impurity index), and make a simple stem plot of the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feat_tr[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.stem(range(10), importances[indices[:10]], align=\"center\")\n",
    "plt.xticks(range(10), feat_tr[indices[:10]], rotation=\"vertical\")\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance\n",
    "\n",
    "Compute and print the confusion matrix.\n",
    "\n",
    "### Recap\n",
    "\n",
    "In this example, the first row is class -1, so the confusion matrix will look like:\n",
    "\n",
    "|      |  |  Predicted  |    |\n",
    "|------|-----------|----|----|\n",
    "|      |           | -1 | 1  |\n",
    "| True | -1        | TN | FP |\n",
    "|      | 1         | FN | TP |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_ts) # use the trained RandomForestClassifier to predict labels on x_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_ts, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of class -1 test samples (AN = All Negatives) should be equal to the sum of the first row of the confusion matrix, i.e., TN + FP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_ts==-1) # total number of \"class -1\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for class 1, i.e., AP = All Positives = TP + FN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_ts==1) # total number of \"class +1\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Accuracy, remembering/using the formula: \n",
    "\n",
    "ACC = (TN + TP) / (TN + TP + FN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TN and TP are on the main diagonal of our conf Numpy array. The denominator is equivalent to the overall sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(conf[0,0] + conf[1,1])/y_ts.shape[0] # y_ts.shape[0] is the sample size of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily Scikit-learn provides a quite handy alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_ts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the MCC (again, Scikit-learn comes to our help):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef(y_ts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "But how do I know if this model performs similarly well on a similar/unseen data set?\n",
    "\n",
    "In other words, does this model generalize beyond its training set?\n",
    "\n",
    "This is why data partitioning techniques are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple random split of a dataset in two groups (hold-out strategy), leaving 25% of the samples for model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tr, y_tr, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Split the training dataset, x_tr, with a 5-fold partitioning schema, keeping the class label proportions across folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "N = skf.get_n_splits(x_tr, y_tr)\n",
    "\n",
    "for i, (idx_tr, idx_ts) in enumerate(skf.split(x_tr, y_tr)):\n",
    "    print(\"Fold %d / %d\" % (i+1, N))\n",
    "    X_train, Y_train = x_tr[idx_tr], y_tr[idx_tr]\n",
    "    X_test, Y_test = x_tr[idx_ts], y_tr[idx_ts]\n",
    "    print(\"TRAIN size:\", X_train.shape[0])\n",
    "    print(\"-- class 1:\", np.sum(Y_train==1), \"class -1:\", np.sum(Y_train==-1))\n",
    "    print(\"TEST size:\", X_test.shape[0])\n",
    "    print(\"-- class 1:\", np.sum(Y_test==1), \"class -1:\", np.sum(Y_test==-1))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing a basic Data Analysis Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, we implement a 10x 5-fold Cross-validation schema with a simple feature ranking. \n",
    "\n",
    "For each CV iteration, a Random Forest model is trained on the training portion of the data, then features are ranked according to the Random Forest importances; a series of Random Forest models are built upon an increasing number of the ranked features (i.e., 1, 5, 10, etc.) and evaluated on the test data in terms of MCC.\n",
    "\n",
    "The average MCC over the 10x5 CV iterations is computed for the different feature set sizes. We choose the feature set size that maximizes the average MCC.\n",
    "\n",
    "This basic example is meant as a starting point for building more complex pipelines, i.e., with more feature steps, (bootstrapped) confidence intervals for MCC, computation of a unified ranked feature list (as in Jurman et al., _Bioinformatics_, 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dap.png\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_N = 10 # number of CV iterations\n",
    "CV_K = 5 # number of CV folds\n",
    "FEATURE_STEPS = [1, 5, 10, 25, 50, 100]\n",
    "# prepare output MCC array\n",
    "MCC = np.empty((CV_K*CV_N, len(FEATURE_STEPS)))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n in range(CV_N):\n",
    "    print(\"~~~ Iteration %d ~~~\" % (n+1))\n",
    "    skf = StratifiedKFold(n_splits=CV_K, shuffle=True, random_state=n)\n",
    "    for i, (idx_tr, idx_ts) in enumerate(skf.split(x_tr, y_tr)):\n",
    "        print(\"Fold %d\" % (i+1))\n",
    "        X_train, Y_train = x_tr[idx_tr], y_tr[idx_tr]\n",
    "        X_test, Y_test = x_tr[idx_ts], y_tr[idx_ts]\n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=500, random_state=n)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        ranking = np.argsort( clf.feature_importances_ )[::-1]\n",
    "        \n",
    "        for j, s in enumerate(FEATURE_STEPS):\n",
    "            v = ranking[:s] # consider the top s ranked features\n",
    "            X_tr_fs, X_ts_fs = X_train[:, v], X_test[:, v] # extract them from internal train and test data\n",
    "            clf.fit(X_tr_fs, Y_train) # train a classifier on the reduced train dataset\n",
    "            YP = clf.predict(X_ts_fs) # predict on the reduced test dataset\n",
    "            MCC[(n*CV_K)+i, j] = matthews_corrcoef(Y_test, YP) # evaluate the model performance\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"MCC_CV\", MCC)\n",
    "# MCC = np.load(\"MCC_CV.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MCC_avg = np.mean(MCC, axis=0)\n",
    "MCC_max = np.max(MCC_avg)\n",
    "n_feats = FEATURE_STEPS[np.argmax(MCC_avg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average MCC for each feature step\n",
    "for nf, mcc in zip(FEATURE_STEPS, MCC_avg):\n",
    "    print(\"nf = %d, MCC = %.2f\" % (nf, mcc))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Best MCC = %.2f with %d features\" % (MCC_max, n_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Average MCC\")\n",
    "plt.plot(FEATURE_STEPS, MCC_avg, 'o-')\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"MCC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
